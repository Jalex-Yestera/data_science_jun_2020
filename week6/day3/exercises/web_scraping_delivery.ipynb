{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595506183833",
   "display_name": "Python 3.6.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From HTML\n",
    "\n",
    "*Using only beautiful soap*\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Each row of the dataframe must have in different columns:\n",
    "\n",
    "- The name of the title\n",
    "- The id of the div where is the value scraped. If there is not id, then the value is must be numpy.nan\n",
    "- The name of the tag where is the value scraped.\n",
    "- The next scraped values in different rows: \n",
    "    - The value: \"Este es el segundo párrafo\"  --> Row 1\n",
    "    - The url https://pagina1.xyz/ --> Row 2\n",
    "    - The url https://pagina4.xyz/ --> Row 3\n",
    "    - The url https://pagina5.xyz/ --> Row 4\n",
    "    - The value \"links footer-links\" --> Row 5\n",
    "    - The value \"Este párrafo está en el footer\" --> Row 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Página de prueba</title>\n",
    "</head>\n",
    "<body>\n",
    "<div id=\"main\" class=\"full-width\">\n",
    "    <h1>El título de la página</h1>\n",
    "    <p>Este es el primer párrafo</p>\n",
    "    <p>Este es el segundo párrafo</p>\n",
    "    <div id=\"innerDiv\">\n",
    "        <div class=\"links\">\n",
    "            <a href=\"https://pagina1.xyz/\">Enlace 1</a>\n",
    "            <a href=\"https://pagina2.xyz/\">Enlace 2</a>\n",
    "        </div>\n",
    "        <div class=\"right\">\n",
    "            <div class=\"links\">\n",
    "                <a href=\"https://pagina3.xyz/\">Enlace 3</a>\n",
    "                <a href=\"https://pagina4.xyz/\">Enlace 4</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div id=\"footer\">\n",
    "        <!-- El footer -->\n",
    "        <p>Este párrafo está en el footer</p>\n",
    "        <div class=\"links footer-links\">\n",
    "            <a href=\"https://pagina5.xyz/\">Enlace 5</a>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['main', 'innerDiv', 'footer']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "\n",
    "div_id = [re.search('\"\\w+\"', e).group()[1:-1] for e in re.findall(\"id=.*\",str(soup.contents[0]))]\n",
    "\n",
    "div_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Página de prueba'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Get the title\n",
    "Titulo =soup.title.string\n",
    "Titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<p>Este es el segundo párrafo</p>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Get 2nd paragraph\n",
    "Seg_par = soup.findAll('p')[1]\n",
    "Seg_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['https://pagina1.xyz/',\n 'https://pagina2.xyz/',\n 'https://pagina3.xyz/',\n 'https://pagina4.xyz/',\n 'https://pagina5.xyz/']"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Get all text between tags 'a' in a list\n",
    "Urls= soup.findAll('a')\n",
    "Lst_url =[]\n",
    "# Create for loop to put links in the list\n",
    "for url in Urls:\n",
    "    link = url.get('href')\n",
    "    Lst_url.append(link)\n",
    "Lst_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'links footer-links'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "lista = re.findall(\"class=.*\",str(soup.contents[0]))\n",
    "Link_F = lista[4]\n",
    "Link_F = Link_F[7:-2]\n",
    "# print(lista)\n",
    "Link_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Este párrafo está en el footer'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Get 2nd paragraph\n",
    "Estepa = soup.findAll('p')[2].text\n",
    "Estepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['p', 'p', 'a', 'a', 'a', 'a', 'p', 'a']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "a_tags= []\n",
    "for tag in soup.find_all(['a','p']):\n",
    "    a_tags.append(tag.name)\n",
    "a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Página de prueba\nEste es el segundo párrafo\n['main', 'innerDiv', 'footer']\n['https://pagina1.xyz/', 'https://pagina2.xyz/', 'https://pagina3.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/']\nlinks footer-links\nEste párrafo está en el footer\n['p', 'p', 'a', 'a', 'a', 'a', 'p', 'a']\n"
    }
   ],
   "source": [
    "Values = [Seg_par.text, Lst_url[0], Lst_url[3], Lst_url[4] ]\n",
    "print(Titulo)\n",
    "print(Seg_par.text)\n",
    "print(div_id)\n",
    "print(Lst_url)\n",
    "print(Link_F)\n",
    "print(Estepa)\n",
    "print(a_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             Titulo    Div_id  Tag                           Value\n0  Página de prueba      main    p      Este es el segundo párrafo\n1  Página de prueba  innerDiv    a            https://pagina1.xyz/\n2  Página de prueba  innerDiv    a            https://pagina4.xyz/\n3  Página de prueba    footer    a            https://pagina5.xyz/\n4  Página de prueba    footer  Nan              links footer-links\n5  Página de prueba    footer    p  Este párrafo está en el footer",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Titulo</th>\n      <th>Div_id</th>\n      <th>Tag</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Página de prueba</td>\n      <td>main</td>\n      <td>p</td>\n      <td>Este es el segundo párrafo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Página de prueba</td>\n      <td>innerDiv</td>\n      <td>a</td>\n      <td>https://pagina1.xyz/</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Página de prueba</td>\n      <td>innerDiv</td>\n      <td>a</td>\n      <td>https://pagina4.xyz/</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Página de prueba</td>\n      <td>footer</td>\n      <td>a</td>\n      <td>https://pagina5.xyz/</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Página de prueba</td>\n      <td>footer</td>\n      <td>Nan</td>\n      <td>links footer-links</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Página de prueba</td>\n      <td>footer</td>\n      <td>p</td>\n      <td>Este párrafo está en el footer</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "data = {'Titulo': [Titulo, Titulo, Titulo, Titulo, Titulo, Titulo], 'Div_id':[ 'main', 'innerDiv','innerDiv','footer','footer','footer'], 'Tag':['p','a','a','a','Nan','p',],'Value':[Seg_par.text, Lst_url[0], Lst_url [3],Lst_url[4],Link_F,Estepa]}\n",
    "pd.DataFrame.from_dict(data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. From Amazon\n",
    "\n",
    "*Using  beautiful soap and/or regex*\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Using product pages from Amazon, do the following: \n",
    "\n",
    "- Get the product name from the web and save it in a column called \"item_name\"\n",
    "- Get the price from the web and save it in a column called \"item_price\"\n",
    "\n",
    "While you are doing the exercise, document the steps you are doing. Try to do the program for generic pages. If you cannot do it generic, explain the reasons. \n",
    "\n",
    "-------------------------------\n",
    "\n",
    "**Example:** \n",
    "\n",
    "url = https://www.amazon.es/Tommy-Hilfiger-UM0UM00054-Camiseta-Hombre/dp/B01MYD0T1F/ref=sr_1_1?dchild=1&pf_rd_p=58224bec-cac9-4dd2-a42a-61b1db609c2d&pf_rd_r=VZQ1JTQXFVRZ9E9VSKX4&qid=1595364419&s=apparel&sr=1-1\n",
    "\n",
    "*item_name* --> \"Tommy Hilfiger Logo Camiseta de Cuello Redondo,Perfecta para El Tiempo Libre para Hombre\"\n",
    "\n",
    "*item_price* --> [[18,99 € - 46,59 €]] or one of the options.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse Amazon to find something funny\n",
    "url = 'https://www.amazon.es/joven-haberte-comprado-sart%C3%A9n-antiadherente/dp/1640015426/ref=sr_1_20?dchild=1&keywords=libros+para+colorear+adultos&qid=1595668292&sr=8-20'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hacemos la sopa \n",
    "page = requests.get(url, headers={'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'})\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "html\nhead\nmeta\nmeta\nmeta\ntitle\nmeta\nlink\nscript\nbody\ndiv\ndiv\ndiv\ni\ndiv\ndiv\ni\nh4\np\ndiv\ndiv\ndiv\nform\ninput\ninput\ndiv\ndiv\ndiv\nh4\ndiv\nimg\ndiv\ndiv\ndiv\ndiv\na\ninput\ndiv\ndiv\nspan\nspan\nbutton\ndiv\ndiv\ndiv\na\nspan\nspan\nspan\nspan\na\ndiv\nscript\nnoscript\nimg\nscript\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['html',\n 'head',\n 'meta',\n 'meta',\n 'meta',\n 'title',\n 'meta',\n 'link',\n 'script',\n 'body',\n 'div',\n 'div',\n 'div',\n 'i',\n 'div',\n 'div',\n 'i',\n 'h4',\n 'p',\n 'div',\n 'div',\n 'div',\n 'form',\n 'input',\n 'input',\n 'div',\n 'div',\n 'div',\n 'h4',\n 'div',\n 'img',\n 'div',\n 'div',\n 'div',\n 'div',\n 'a',\n 'input',\n 'div',\n 'div',\n 'span',\n 'span',\n 'button',\n 'div',\n 'div',\n 'div',\n 'a',\n 'span',\n 'span',\n 'span',\n 'span',\n 'a',\n 'div',\n 'script',\n 'noscript',\n 'img',\n 'script']"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Explore tags\n",
    "Tag_list = []\n",
    "for tag in soup.find_all(True):\n",
    "    Tag_list.append(tag.name)\n",
    "    print(tag.name)\n",
    "Tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "link = soup.find_all(id=\"productTitle\")\n",
    "#<span id=\"productTitle\" class=\"a-size-extra-large\">\n",
    "#Un día eres joven y al otro eres feliz por haberte comprado una sartén antiadherente: Un libro de colorear para adultos\n",
    "#</span>\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for child in soup.body.descendants:\n",
    "  #  print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'child' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ee02c885b5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchild\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'child' is not defined"
     ]
    }
   ],
   "source": [
    "child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-196f1f56bdb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# necesitamos find mas ingredientes para la soup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"priceblock_ourprice\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "# necesitamos find mas ingredientes para la soup \n",
    "title = soup.find(id=\"Title\").get_text()\n",
    "price = soup.find(id=\"priceblock_ourprice\").get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echamos la url a la olla para hacer la sopa\n",
    "url = 'https://www.amazon.es/joven-haberte-comprado-sart%C3%A9n-antiadherente/dp/1640015426/ref=sr_1_20?dchild=1&keywords=libros+para+colorear+adultos&qid=1595668292&sr=8-20'\n",
    "\n",
    "def get_page_contents(url):\n",
    "    page = requests.get(url, headers={\"Accept-Language\": \"en-US\"})\n",
    "    return BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "sopita = get_page_contents(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "Product_name = sopita.find_all(id_='productTitle')\n",
    "Product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_Text'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b2853cef5205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"productTitle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_Text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"priceblock_ourprice\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_Text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_Text'"
     ]
    }
   ],
   "source": [
    "# Encontré el ejercicio resuelto en towardsdatascience.\n",
    "# Importamos las librerías necesarias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import smtplib\n",
    "\n",
    "# Relleno las headers con mis datos obtenidos usando xhaus\n",
    "headers = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
    "\n",
    "# Sustituyo la url por algo que mole\n",
    "url = 'https://www.amazon.de/gp/product/B0756CYWWD/ref=as_li_tl?ie=UTF8&tag=idk01e-21&camp=1638&creative=6742&linkCode=as2&creativeASIN=B0756CYWWD&linkId=18730d371b945bad11e9ea58ab9d8b32'\n",
    "\n",
    "\n",
    "# con requests nos traemos el contenido de la web con nuestras cabeceras\n",
    "page = requests.get(url, headers=headers)\n",
    "# hacemos la sopa\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "title = soup.find(id=\"productTitle\").get_Text()\n",
    "price = soup.find(id=\"priceblock_ourprice\").get_Text()\n",
    "sep = ','\n",
    "con_price = price.split(sep, 1)[0]\n",
    "converted_price = int(con_price.replace('.', ''))\n",
    "\n",
    "# price\n",
    "print(title.strip())\n",
    "print(converted_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Relleno las headers con mis datos obtenidos usando xhaus\n",
    "headers = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
    "\n",
    "# Sustituyo la url por algo que mole\n",
    "url = 'https://www.amazon.es/joven-haberte-comprado-sart%C3%A9n-antiadherente/dp/1640015426/ref=sr_1_20?dchild=1&keywords=libros+para+colorear+adultos&qid=1595668292&sr=8-20'\n",
    "\n",
    "\n",
    "# con requests nos traemos el contenido de la web con nuestras cabeceras\n",
    "page = requests.get(url, headers=headers)\n",
    "# hacemos la sopa\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "type(soup)\n",
    "title = soup.find(id=\"productTitle\")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-40dabd2b9acb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mamazon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-40dabd2b9acb>\u001b[0m in \u001b[0;36mamazon\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"productTitle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"priceblock_ourprice\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-agent\": 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "\n",
    "URL = 'https://www.amazon.de/gp/product/B0756CYWWD/ref=as_li_tl?ie=UTF8&tag=idk01e-21&camp=1638&creative=6742&linkCode=as2&creativeASIN=B0756CYWWD&linkId=18730d371b945bad11e9ea58ab9d8b32'\n",
    "def amazon():\n",
    "\n",
    "    page = requests.get(URL, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"productTitle\").get_text()\n",
    "    price = soup.find(id=\"priceblock_ourprice\").get_text()\n",
    "    sep = ','\n",
    "    con_price = price.split(sep, 1)[0]\n",
    "    converted_price = int(con_price.replace('.', ''))\n",
    "\n",
    "    # price\n",
    "    print(title.strip())\n",
    "    print(converted_price)\n",
    "\n",
    "amazon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}